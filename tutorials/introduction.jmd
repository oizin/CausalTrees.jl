---
title: An Introduction to CausalTrees.jl
author: Ois√≠n Fitzgerald
---

#### Causal Trees

##### Background

CausalTrees.jl allows the estimation of the conditional average treatment effect function $\tau(x) = E(Y(1)-Y(0)|X=x)$
where $Y(a)$ is the potential outcome of the random variable Y under treatment $A = a$, while $X$ is a set of $p$ features observed at values $x$.
We can link our observed outcome, treatment assignment and the potential outcomes through $Y = AY(1) + (1 - A)Y(0)$.
A treatment effect is heterogeneous if the value of $\tau(x)$ varies as we consider different values of $X = x$.
Under homogeneous treatment $\tau(x) = \tau$, a constant, a situation that while in many applications (e.g. medicine) is not plausible,
may offer the best estimate due to weak heterogeneity and noisy data.
Below we consider the case of a continuous outcomes $Y$ and binary treatment $A \in {0,1}$.

The causal tree (CT) approach to estimating $\tau(x)$ follows the  recursive partitioning approach to decision tree learning
commonly applied (e.g. CART) in the fields of machine learning and statistics.
The feature space $X$ is partitioned into $J$ regions $S_j$ within each of which the treatment effect is well
approximated (wrt to some error function) by a constant $\tau_j$. This results in $\tau(x) = \sum_j 1_{(x \in S_j)}\tau_j$, where
1_{a} is an indicator function that returns 1 if a is true and 0 otherwise.

##### Previous research

CausalTrees.jl is an implementation of the recursive partitioning algorithms proposed by Su et al (2009) and Athey & Imbens (2016).

Other implementations of this or similar methodology include:

* grf: [https://grf-labs.github.io/grf/](https://grf-labs.github.io/grf/)

##### A simple example

Firstly we'll make some data where the treatment effect is dependent on the observed input features.
In this case higher values of X1 (in particular), X2 and X3 predict a larger treatment effect.
This mimics a medical settings where the treatment is most beneficial for severely ill patients
but potentially detrimental to less ill patients, think of X1 as a severity score.

```julia
# set seed for reproducibility
import Random
Random.seed!(1234)
# Generate RCT data
n = 200
# covariates
x = randn((n,6))
# treatment group
w = rand([0,1],n)
# The expected treatment effect: E(Y1-Y0|X)
tau =  min.(2 .+ 3.35 .* x[:,1] + 1.2 .* x[:,1].^2 + 0.3 .* x[:,1].^3 + 0.4 .* x[:,2] .* x[:,3],25)
# The potential outcomes
y0 = 1 .* x[:,1] + x[:,2] + randn(n)
y1 = 1 .* x[:,1] + x[:,2] + tau + randn(n)
# individual treatment effect
truth = y1 .- y0
# observed outcome
y = (1 .- w) .* y0 + w .* y1
y = reshape(y,n);
```
As we can see from the plot the heterogeneity is quite (unrealistically) strong in this example.

```julia
# plot
using Plots; pyplot()
using LaTeXStrings
scatter(x[w .== 0,1],truth[w .== 0],label ="W = 0",color= "pink",xlab="X1",ylab=L"\tau(x) + e",
title="Individual treatment effects")
scatter!(x[w .== 1,1],truth[w .== 1],label ="W = 1",color= "blue",xlab="X1",ylab=L"\tau(x) + e",
legendtitle="Treatment group")

```
We can fit a causal tree using `CausalTrees.causal_tree` specifying, along with the input data, that no further
partitioning of the data should be considered when a partition contains 30 samples.

```julia
using CausalTrees
# fit tree and predict
t1 = causal_tree(x,w,y,mse_tau,min_leaf_size=30)
CausalTrees.print(t1)
tau_hat = predict(t1,x)
# calculate average treatment effect
using Statistics
y1_hat = mean(y[w .== 1])
y0_hat = mean(y[w .== 0])
ate_hat = y1_hat - y0_hat
println("average treatment effect: ",round(ate_hat,digits=4))
```
If we compare the CT treatment estimations to the plot of the true treatment effects and
the ATE we see that the tree has done a reasonably successful job approximating $\tau(x)$
using local estimates.

```julia
scatter(x[:,1],tau,label= "",xlab="X1",ylab=L"\tau(x)",markeralpha=0.5,
markerstrokealpha=0.0,markerstrokecolor=:blue,title="Observed vs. estimated treatment effects")
Plots.scatter!(x[:,1],tau_hat,label="CT",markershape=:xcross,markercolor=:red,
markerstrokecolor=:red,markersize=2)
hline!([ate_hat],label="ATE")
```
We can also plot the fitted CT, demonstrating the explanatory power of decision trees.

```julia
CausalTrees.plot(t1,leaf_size=5,split_xsize=2,split_ysize=1)
```

As suggested by the above figure in this case the CT results in lower estimation
error than reliance on the ATE.

```julia
rmse(y, y_) = sqrt(mean((y .- y_).^2))
println("root mean square error (ate): ",round(rmse(ate_hat,tau),digits=4))
println("root mean square error (CT): ",round(rmse(tau_hat,tau),digits=4))
```
##### A small simulation

Much weaker heterogeneity.

Consider performance vs. level 0 benchmark.

```julia
# data generation function
function gen_data(n=200,pnoise=3)
  # covariates
  x = randn((n,pnoise+3))
  # treatment group
  w = rand([0,1],n)
  # The expected treatment effect: E(Y1-Y0|X)
  tau =  min.(2 .+ 2.35 .* x[:,1] + 0.3 .* x[:,1].^2 + 0.1 .* x[:,1].^3 - 0.4 .* x[:,2] .* x[:,3],25)
  # The potential outcomes
  y0 = 1 .* x[:,1] + x[:,2] + randn(n)
  y1 = 1 .* x[:,1] + x[:,2] + tau + randn(n)
  # individual treatment effect
  truth = y1 .- y0
  # observed outcome
  y = (1 .- w) .* y0 + w .* y1
  y = reshape(y,n)
  return(x,w,y,tau)
end

# example
x,w,y,tau = gen_data()
l = @layout [a ; b c]
p1 = scatter(x[:,1],tau,label="",title="Treatment effect",ylab=L"\tau(x)",xlab="X1")
p2 = histogram(y[w .== 1],label="",title="Observed outome:\nTreated (W = 1)",xlab="Y")
p3 = histogram(y[w .== 0],tau,label="",title="Observed outome:\nControl (W = 0)",xlab="Y")
Plots.plot(p1,p2,p3,layout = l)
```

```julia
Random.seed!(1234)

M = 300

ct_rmse = zeros(M)
ate_rmse = zeros(M)
for m in 1:M
  x,w,y,tau = gen_data()
  t1 = causal_tree(x,w,y,mse_tau,min_leaf_size=30)
  tau_hat = predict(t1,x)
  y1_hat = mean(y[w .== 1])
  y0_hat = mean(y[w .== 0])
  ate_hat = y1_hat - y0_hat
  ate_rmse[m] = rmse(ate_hat,tau)
  ct_rmse[m] = rmse(tau_hat,tau)
end

println("CT --  Mean(RMSE): ",round(mean(ct_rmse),digits=4)," Std(RMSE): ",round(std(ct_rmse),digits=4))
println("ATE -- Mean(RMSE): ",round(mean(ate_rmse),digits=4)," Std(RMSE): ",round(std(ate_rmse),digits=4))

```

```julia
h1 = histogram(ct_rmse,xlim=[0,6.0],title="Causal Tree: RMSE",label="",color=:pink)
h2 = histogram(ate_rmse,xlim=[0,6.0],title="ATE: RMSE",label="")
Plots.plot(h1,h2,layout = (2,1))
```

The ATE is a low variance high bias estimate, but CT is a

#### References

Athey, S., & Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27), 7353-7360.

Su, X., Tsai, C. L., Wang, H., Nickerson, D. M., & Li, B. (2009). Subgroup analysis via recursive partitioning. Journal of Machine Learning Research, 10(Feb), 141-158.
